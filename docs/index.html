<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
    <meta name="generator" content="pdoc 0.8.1" />
    <title>nujo API documentation</title>
    <meta name="description" content="" />
    <link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
    <link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
    <style>
        .flex {
            display: flex !important
        }

        body {
            line-height: 1.5em
        }

        #content {
            padding: 20px
        }

        #sidebar {
            padding: 30px;
            overflow: hidden
        }

        #sidebar>*:last-child {
            margin-bottom: 2cm
        }

        .http-server-breadcrumbs {
            font-size: 130%;
            margin: 0 0 15px 0
        }

        #footer {
            font-size: .75em;
            padding: 5px 30px;
            border-top: 1px solid #ddd;
            text-align: right
        }

        #footer p {
            margin: 0 0 0 1em;
            display: inline-block
        }

        #footer p:last-child {
            margin-right: 30px
        }

        h1,
        h2,
        h3,
        h4,
        h5 {
            font-weight: 300
        }

        h1 {
            font-size: 2.5em;
            line-height: 1.1em
        }

        h2 {
            font-size: 1.75em;
            margin: 1em 0 .50em 0
        }

        h3 {
            font-size: 1.4em;
            margin: 25px 0 10px 0
        }

        h4 {
            margin: 0;
            font-size: 105%
        }

        a {
            color: #058;
            text-decoration: none;
            transition: color .3s ease-in-out
        }

        a:hover {
            color: #e82
        }

        .title code {
            font-weight: bold
        }

        h2[id^="header-"] {
            margin-top: 2em
        }

        .ident {
            color: #900
        }

        pre code {
            background: #f8f8f8;
            font-size: .8em;
            line-height: 1.4em
        }

        code {
            background: #f2f2f1;
            padding: 1px 4px;
            overflow-wrap: break-word
        }

        h1 code {
            background: transparent
        }

        pre {
            background: #f8f8f8;
            border: 0;
            border-top: 1px solid #ccc;
            border-bottom: 1px solid #ccc;
            margin: 1em 0;
            padding: 1ex
        }

        #http-server-module-list {
            display: flex;
            flex-flow: column
        }

        #http-server-module-list div {
            display: flex
        }

        #http-server-module-list dt {
            min-width: 10%
        }

        #http-server-module-list p {
            margin-top: 0
        }

        .toc ul,
        #index {
            list-style-type: none;
            margin: 0;
            padding: 0
        }

        #index code {
            background: transparent
        }

        #index h3 {
            border-bottom: 1px solid #ddd
        }

        #index ul {
            padding: 0
        }

        #index h4 {
            margin-top: .6em;
            font-weight: bold
        }

        @media (min-width:200ex) {
            #index .two-column {
                column-count: 2
            }
        }

        @media (min-width:300ex) {
            #index .two-column {
                column-count: 3
            }
        }

        dl {
            margin-bottom: 2em
        }

        dl dl:last-child {
            margin-bottom: 4em
        }

        dd {
            margin: 0 0 1em 3em
        }

        #header-classes+dl>dd {
            margin-bottom: 3em
        }

        dd dd {
            margin-left: 2em
        }

        dd p {
            margin: 10px 0
        }

        .name {
            background: #eee;
            font-weight: bold;
            font-size: .85em;
            padding: 5px 10px;
            display: inline-block;
            min-width: 40%
        }

        .name:hover {
            background: #e0e0e0
        }

        .name>span:first-child {
            white-space: nowrap
        }

        .name.class>span:nth-child(2) {
            margin-left: .4em
        }

        .inherited {
            color: #999;
            border-left: 5px solid #eee;
            padding-left: 1em
        }

        .inheritance em {
            font-style: normal;
            font-weight: bold
        }

        .desc h2 {
            font-weight: 400;
            font-size: 1.25em
        }

        .desc h3 {
            font-size: 1em
        }

        .desc dt code {
            background: inherit
        }

        .source summary,
        .git-link-div {
            color: #666;
            text-align: right;
            font-weight: 400;
            font-size: .8em;
            text-transform: uppercase
        }

        .source summary>* {
            white-space: nowrap;
            cursor: pointer
        }

        .git-link {
            color: inherit;
            margin-left: 1em
        }

        .source pre {
            max-height: 500px;
            overflow: auto;
            margin: 0
        }

        .source pre code {
            font-size: 12px;
            overflow: visible
        }

        .hlist {
            list-style: none
        }

        .hlist li {
            display: inline
        }

        .hlist li:after {
            content: ',\2002'
        }

        .hlist li:last-child:after {
            content: none
        }

        .hlist .hlist {
            display: inline;
            padding-left: 1em
        }

        img {
            max-width: 100%
        }

        .admonition {
            padding: .1em .5em;
            margin-bottom: 1em
        }

        .admonition-title {
            font-weight: bold
        }

        .admonition.note,
        .admonition.info,
        .admonition.important {
            background: #aef
        }

        .admonition.todo,
        .admonition.versionadded,
        .admonition.tip,
        .admonition.hint {
            background: #dfd
        }

        .admonition.warning,
        .admonition.versionchanged,
        .admonition.deprecated {
            background: #fd4
        }

        .admonition.error,
        .admonition.danger,
        .admonition.caution {
            background: lightpink
        }
    </style>
    <style media="screen and (min-width: 700px)">
        @media screen and (min-width:700px) {
            #sidebar {
                width: 30%;
                height: 100vh;
                overflow: auto;
                position: sticky;
                top: 0
            }

            #content {
                width: 70%;
                max-width: 100ch;
                padding: 3em 4em;
                border-left: 1px solid #ddd
            }

            pre code {
                font-size: 1em
            }

            .item .name {
                font-size: 1em
            }

            main {
                display: flex;
                flex-direction: row-reverse;
                justify-content: flex-end
            }

            .toc ul ul,
            #index ul {
                padding-left: 1.5em
            }

            .toc>ul>li {
                margin-top: .5em
            }
        }
    </style>
    <style media="print">
        @media print {
            #sidebar h1 {
                page-break-before: always
            }

            .source {
                display: none
            }
        }

        @media print {
            * {
                background: transparent !important;
                color: #000 !important;
                box-shadow: none !important;
                text-shadow: none !important
            }

            a[href]:after {
                content: " ("attr(href) ")";
                font-size: 90%
            }

            a[href][title]:after {
                content: none
            }

            abbr[title]:after {
                content: " ("attr(title) ")"
            }

            .ir a:after,
            a[href^="javascript:"]:after,
            a[href^="#"]:after {
                content: ""
            }

            pre,
            blockquote {
                border: 1px solid #999;
                page-break-inside: avoid
            }

            thead {
                display: table-header-group
            }

            tr,
            img {
                page-break-inside: avoid
            }

            img {
                max-width: 100% !important
            }

            @page {
                margin: 0.5cm
            }

            p,
            h2,
            h3 {
                orphans: 3;
                widows: 3
            }

            h1,
            h2,
            h3,
            h4,
            h5,
            h6 {
                page-break-after: avoid
            }
        }
    </style>
</head>

<body>
    <main>
        <article id="content">
            <header>
                <h1 class="title">Package <code>nujo</code></h1>
            </header>
            <section id="section-intro">
                <details class="source">
                    <summary>
                        <span>Expand source code</span>
                    </summary>
                    <pre><code class="python">from nujo.autodiff import Function, Tensor, no_diff
from nujo.flow import Flow
from nujo.init import *
from nujo.math import *

__all__ = [
    &#39;Function&#39;,
    &#39;Tensor&#39;,
    &#39;no_diff&#39;,
    &#39;Flow&#39;,
]

__version__ = &#39;0.2.0&#39;</code></pre>
                </details>
            </section>
            <section>
                <h2 class="section-title" id="header-submodules">Sub-modules</h2>
                <dl>
                    <dt><code class="name"><a title="nujo.autodiff" href="autodiff/index.html">nujo.autodiff</a></code>
                    </dt>
                    <dd>
                        <div class="desc">
                            <p>nujo's core Reverse-mode Automatic Differentiation module</p>
                        </div>
                    </dd>
                    <dt><code class="name"><a title="nujo.flow" href="flow.html">nujo.flow</a></code></dt>
                    <dd>
                        <div class="desc">
                            <p>a computation Flow</p>
                        </div>
                    </dd>
                    <dt><code class="name"><a title="nujo.init" href="init/index.html">nujo.init</a></code></dt>
                    <dd>
                        <div class="desc">
                            <p>Tensor initializers …</p>
                        </div>
                    </dd>
                    <dt><code class="name"><a title="nujo.math" href="math/index.html">nujo.math</a></code></dt>
                    <dd>
                        <div class="desc">
                            <p>nujo's core mathematical functionality</p>
                        </div>
                    </dd>
                    <dt><code class="name"><a title="nujo.nn" href="nn/index.html">nujo.nn</a></code></dt>
                    <dd>
                        <div class="desc">
                            <p>nujo's Neural Network module …</p>
                        </div>
                    </dd>
                    <dt><code
                            class="name"><a title="nujo.objective" href="objective/index.html">nujo.objective</a></code>
                    </dt>
                    <dd>
                        <div class="desc">
                            <p>nujo's objective functions module …</p>
                        </div>
                    </dd>
                    <dt><code class="name"><a title="nujo.optim" href="optim/index.html">nujo.optim</a></code></dt>
                    <dd>
                        <div class="desc">
                            <p>nujo's optimization module …</p>
                        </div>
                    </dd>
                    <dt><code class="name"><a title="nujo.utils" href="utils/index.html">nujo.utils</a></code></dt>
                    <dd>
                        <div class="desc">
                            <p>nujo utils</p>
                        </div>
                    </dd>
                </dl>
            </section>
            <section>
            </section>
            <section>
            </section>
            <section>
                <h2 class="section-title" id="header-classes">Classes</h2>
                <dl>
                    <dt id="nujo.Flow"><code class="flex name class">
<span>class <span class="ident">Flow</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
                    <dd>
                        <div class="desc">
                            <p>A computation Flow</p>
                            <p>A Flow is just a sequance of functions (addition, multiplication, etc.)
                                that are grouped in a single object (Flow) and can be applied on a tensor.</p>
                            <p>Each nujo Flow has a list of flow objects (<code>subflows</code>) that
                                a tensor will pass through when the Flow is called on that tensor.</p>
                            <p>This allows the chaining of flows.</p>
                            <h2 id="parameters">Parameters:</h2>
                            <ul>
                                <li>name : string, name of the current flow</li>
                                <li>subflows : list of flows</li>
                            </ul>
                        </div>
                        <details class="source">
                            <summary>
                                <span>Expand source code</span>
                            </summary>
                            <pre><code class="python">class Flow(metaclass=_FlowSetup):
    &#39;&#39;&#39; A computation Flow

    A Flow is just a sequance of functions (addition, multiplication, etc.)
    that are grouped in a single object (Flow) and can be applied on a tensor.

    Each nujo Flow has a list of flow objects (`subflows`) that
    a tensor will pass through when the Flow is called on that tensor.

    This allows the chaining of flows.

    Parameters:
    -----------
     - name : string, name of the current flow
     - subflows : list of flows

    &#39;&#39;&#39;
    def __init__(self, name=&#39;Flow&#39;, subflows: List[&#39;Flow&#39;] = []):
        self.name = name
        self.subflows = subflows

        if len(self.subflows):
            self.name = self._generate_supflow_name()

    def _register_parameters(self) -&gt; None:
        &#39;&#39;&#39; Called after Flow.__init__
        &#39;&#39;&#39;

        for flow in self.subflows:
            for prop_name in dir(flow):
                prop = getattr(flow, prop_name)

                if isinstance(prop, Tensor):
                    prop.diff = True

    def _generate_supflow_name(self) -&gt; str:
        return &#39; &gt;&gt; &#39;.join(map(lambda x: x.name, self.subflows))

    def parameters(self) -&gt; Tensor:
        for flow in self.subflows:
            for prop_name in dir(flow):
                prop = getattr(flow, prop_name)

                if isinstance(prop, Tensor):
                    updated = (yield prop)
                    yield
                    if updated is not None:
                        prop &lt;&lt;= updated

    def append(self, *flows: &#39;Flow&#39;) -&gt; &#39;Flow&#39;:
        &#39;&#39;&#39; Flow Append

        Appends a new Flow on top of the current one.

        Parameters:
        -----------
         - flows : varargs, the flows to append, sequantially

        Returns:
        --------
         - flow : Flow, the total computation flow

        &#39;&#39;&#39;

        for flow in flows:
            for subflow in flow:
                self.subflows.append(subflow)

        self.name = self._generate_supflow_name()
        return self

    def pop(self, idx=-1) -&gt; &#39;Flow&#39;:
        &#39;&#39;&#39; Flow Pop

        Removes a flow at a given index, defaults to the last one (-1).

        Parameters:
        -----------
         - idx : integer, index of the flow to remove

        Returns:
        --------
         - flow : Flow, the total computation flow

        &#39;&#39;&#39;

        retflow = self.subflows.pop(idx)
        self.name = self._generate_supflow_name()

        return retflow

    def copy(self) -&gt; &#39;Flow&#39;:
        &#39;&#39;&#39; Make a copy of the current flow
        &#39;&#39;&#39;

        return deepcopy(self)

    @abstractmethod
    def forward(self, *args, **kwargs) -&gt; Tensor:
        &#39;&#39;&#39; Flow Forward

        The flow computation is defined here.

        &#39;&#39;&#39;

        pass

    def __call__(self, *args, **kwargs) -&gt; Tensor:
        output = self[0].forward(*args, **kwargs)

        for subflow in self[1:]:
            output = subflow.forward(output, **kwargs)

        return output

    def __rshift__(self, other: &#39;Flow&#39;) -&gt; &#39;Flow&#39;:
        &#39;&#39;&#39; Chaining operator

        Example:
            &gt;&gt;&gt; a = nj.Flow()
            &gt;&gt;&gt; b = nj.Flow()
            &gt;&gt;&gt; chained_flow = a &gt;&gt; b
            &gt;&gt;&gt; result = chained_flow(...)
            &gt;&gt;&gt; ...

        &#39;&#39;&#39;

        return Flow(subflows=[*self.subflows, *other.subflows])

    def __getitem__(self, key: Union[int, str]) -&gt; &#39;Flow&#39;:
        &#39;&#39;&#39; Get subflows by index/name

        Example:
            &gt;&gt;&gt; a = nj.Flow(&#39;A&#39;)
            &gt;&gt;&gt; b = nj.Flow(&#39;B&#39;)
            &gt;&gt;&gt; chained_flow = a &gt;&gt; b
            &gt;&gt;&gt; chained_flow[0]  # a subflow can be get by index
            &#39;A&#39; (this is the repr for `a`)
            &gt;&gt;&gt; chained_flow[&#39;A&#39;]  # can also be get by name
            &#39;A&#39;

        &#39;&#39;&#39;

        if type(key) is str:
            flow = next((x for x in self.subflows if x.name == key), None)
            if flow is not None:
                return flow
            else:
                raise ValueError(f&#39;Could not find a flow named: {key}&#39;)
        else:
            return self.subflows[key]

    def __iter__(self):
        return iter(self.subflows)

    def __len__(self):
        return len(self.subflows)

    def __repr__(self):
        return &#39;&lt;|&#39; + self.name + &#39;&gt;&#39;</code></pre>
                        </details>
                        <h3>Subclasses</h3>
                        <ul class="hlist">
                            <li><a title="nujo.nn.activations.BinaryStep"
                                    href="nn/activations.html#nujo.nn.activations.BinaryStep">BinaryStep</a></li>
                            <li><a title="nujo.nn.activations.LeakyReLU"
                                    href="nn/activations.html#nujo.nn.activations.LeakyReLU">LeakyReLU</a></li>
                            <li><a title="nujo.nn.activations.ReLU"
                                    href="nn/activations.html#nujo.nn.activations.ReLU">ReLU</a></li>
                            <li><a title="nujo.nn.activations.Sigmoid"
                                    href="nn/activations.html#nujo.nn.activations.Sigmoid">Sigmoid</a></li>
                            <li><a title="nujo.nn.activations.Softmax"
                                    href="nn/activations.html#nujo.nn.activations.Softmax">Softmax</a></li>
                            <li><a title="nujo.nn.activations.Swish"
                                    href="nn/activations.html#nujo.nn.activations.Swish">Swish</a></li>
                            <li><a title="nujo.nn.activations.TanH"
                                    href="nn/activations.html#nujo.nn.activations.TanH">TanH</a></li>
                            <li><a title="nujo.nn.layers.Conv2d" href="nn/layers.html#nujo.nn.layers.Conv2d">Conv2d</a>
                            </li>
                            <li><a title="nujo.nn.layers.Linear" href="nn/layers.html#nujo.nn.layers.Linear">Linear</a>
                            </li>
                            <li>nujo.objective.loss._Loss</li>
                        </ul>
                        <h3>Methods</h3>
                        <dl>
                            <dt id="nujo.Flow.append"><code class="name flex">
<span>def <span class="ident">append</span></span>(<span>self, *flows: Flow) -> <a title="nujo.flow.Flow" href="flow.html#nujo.flow.Flow">Flow</a></span>
</code></dt>
                            <dd>
                                <div class="desc">
                                    <p>Flow Append</p>
                                    <p>Appends a new Flow on top of the current one.</p>
                                    <h2 id="parameters">Parameters:</h2>
                                    <ul>
                                        <li>flows : varargs, the flows to append, sequantially</li>
                                    </ul>
                                    <h2 id="returns">Returns:</h2>
                                    <ul>
                                        <li>flow : Flow, the total computation flow</li>
                                    </ul>
                                </div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def append(self, *flows: &#39;Flow&#39;) -&gt; &#39;Flow&#39;:
    &#39;&#39;&#39; Flow Append

    Appends a new Flow on top of the current one.

    Parameters:
    -----------
     - flows : varargs, the flows to append, sequantially

    Returns:
    --------
     - flow : Flow, the total computation flow

    &#39;&#39;&#39;

    for flow in flows:
        for subflow in flow:
            self.subflows.append(subflow)

    self.name = self._generate_supflow_name()
    return self</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Flow.copy"><code class="name flex">
<span>def <span class="ident">copy</span></span>(<span>self) -> <a title="nujo.flow.Flow" href="flow.html#nujo.flow.Flow">Flow</a></span>
</code></dt>
                            <dd>
                                <div class="desc">
                                    <p>Make a copy of the current flow</p>
                                </div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def copy(self) -&gt; &#39;Flow&#39;:
    &#39;&#39;&#39; Make a copy of the current flow
    &#39;&#39;&#39;

    return deepcopy(self)</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Flow.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *args, **kwargs) -> <a title="nujo.autodiff.tensor.Tensor" href="autodiff/tensor.html#nujo.autodiff.tensor.Tensor">Tensor</a></span>
</code></dt>
                            <dd>
                                <div class="desc">
                                    <p>Flow Forward</p>
                                    <p>The flow computation is defined here.</p>
                                </div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">@abstractmethod
def forward(self, *args, **kwargs) -&gt; Tensor:
    &#39;&#39;&#39; Flow Forward

    The flow computation is defined here.

    &#39;&#39;&#39;

    pass</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Flow.parameters"><code class="name flex">
<span>def <span class="ident">parameters</span></span>(<span>self) -> <a title="nujo.autodiff.tensor.Tensor" href="autodiff/tensor.html#nujo.autodiff.tensor.Tensor">Tensor</a></span>
</code></dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def parameters(self) -&gt; Tensor:
    for flow in self.subflows:
        for prop_name in dir(flow):
            prop = getattr(flow, prop_name)

            if isinstance(prop, Tensor):
                updated = (yield prop)
                yield
                if updated is not None:
                    prop &lt;&lt;= updated</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Flow.pop"><code class="name flex">
<span>def <span class="ident">pop</span></span>(<span>self, idx=-1) -> <a title="nujo.flow.Flow" href="flow.html#nujo.flow.Flow">Flow</a></span>
</code></dt>
                            <dd>
                                <div class="desc">
                                    <p>Flow Pop</p>
                                    <p>Removes a flow at a given index, defaults to the last one (-1).</p>
                                    <h2 id="parameters">Parameters:</h2>
                                    <ul>
                                        <li>idx : integer, index of the flow to remove</li>
                                    </ul>
                                    <h2 id="returns">Returns:</h2>
                                    <ul>
                                        <li>flow : Flow, the total computation flow</li>
                                    </ul>
                                </div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def pop(self, idx=-1) -&gt; &#39;Flow&#39;:
    &#39;&#39;&#39; Flow Pop

    Removes a flow at a given index, defaults to the last one (-1).

    Parameters:
    -----------
     - idx : integer, index of the flow to remove

    Returns:
    --------
     - flow : Flow, the total computation flow

    &#39;&#39;&#39;

    retflow = self.subflows.pop(idx)
    self.name = self._generate_supflow_name()

    return retflow</code></pre>
                                </details>
                            </dd>
                        </dl>
                    </dd>
                    <dt id="nujo.Function"><code class="flex name class">
<span>class <span class="ident">Function</span></span>
<span>(</span><span>*children: Union[nujo.autodiff.tensor.Tensor, numpy.ndarray, List[numbers.Number], numbers.Number], **kwargs)</span>
</code></dt>
                    <dd>
                        <div class="desc">
                            <p>Base Class for functions</p>
                            <p>Functions are applied to tensors. They take multiple
                                tensors as input and produces only one tensor as output.
                                They do NOT change tensors in-place.</p>
                            <p>Functions were also written so they reuse the input/output tensors
                                when possible, which results in the computation graph being:
                                - "Dynamically defined, statically evaluated."
                                taking the best from both worlds.</p>
                            <h2 id="parameters">Parameters:</h2>
                            <ul>
                                <li>children : varargs, the inpute tensors</li>
                                <li>name : string, the name of the function</li>
                            </ul>
                        </div>
                        <details class="source">
                            <summary>
                                <span>Expand source code</span>
                            </summary>
                            <pre><code class="python">class Function(_Node, object):
    &#39;&#39;&#39; Base Class for functions

    Functions are applied to tensors. They take multiple
    tensors as input and produces only one tensor as output.
    They do NOT change tensors in-place.

    Functions were also written so they reuse the input/output tensors
    when possible, which results in the computation graph being:
     - &#34;Dynamically defined, statically evaluated.&#34;
    taking the best from both worlds.

    Parameters:
    -----------
     - children : varargs, the inpute tensors
     - name : string, the name of the function

    &#39;&#39;&#39;

    _func_children_lookup_cache: Dict[str, &#39;Function&#39;] = {}
    &#39;&#39;&#39; Cache used to lookup for functions that may have already been defined
    in the computation graph.

     - key : hash(FuncType) + (children&#39;s identifiers)
     - value : the already defined function which will be reused

    &#39;&#39;&#39;

    _cache_hit = False
    &#39;&#39;&#39; Flag signaling cache hit/miss.
    &#39;&#39;&#39;

    T = TypeVar(&#39;T&#39;, Tensor, ndarray)

    def __init__(self,
                 *children: Union[Tensor, ndarray, List[Number], Number],
                 name=&#39;Function&#39;):

        if self._cache_hit:
            return

        super(Function, self).__init__(*Function._parse_inputs(children),
                                       name=name)

        # This output placeholder is reused when possible
        self._output_placeholder = Tensor(
            None,
            diff=any(x.diff for x in self.children) and modes.DIFF_ENABLED,
            creator=self if modes.DIFF_ENABLED else None,
            name=self._generate_tensor_name())

        if modes.DIFF_ENABLED:  # If graph building is enabled.
            # Allocate space for parent&#39;s output (output placeholder)
            for child in self.children:
                child.parents_outputs.append(self._output_placeholder)

    def __new__(cls, *children: Union[Tensor, ndarray, List[Number], Number],
                **kwargs):
        &#39;&#39;&#39; Used to lookup the cache for an already defined function of
        the current type using the current `children` as inputs, and reuse
        it. If a function satisfying this requirements could not be found,
        a new function is created and added to the cache, in order to be,
        potentially, later reused.

        &#39;&#39;&#39;

        # Only cache functions that are in the computation graph
        if modes.DIFF_ENABLED:
            key = str(hash(cls))  # Inlcude the function type hash in the key
            # Include the inputs&#39; (children&#39;s) identifiers in the key
            key += &#39;&#39;.join((str(x.id) if isinstance(x, Tensor) else str(x)
                            for x in children))

            if key in cls._func_children_lookup_cache:
                cls._cache_hit = True
                return cls._func_children_lookup_cache[key]

            else:
                cls._cache_hit = False
                func = super(Function, cls).__new__(cls)
                cls._func_children_lookup_cache[key] = func
                return func

        # If the functions are not in the computation graph,
        # perform standard python init.
        else:
            cls._cache_hit = False
            return super(Function, cls).__new__(cls)

    @classmethod
    def _parse_inputs(cls, inputs: List[Any]) -&gt; List[Tensor]:
        &#39;&#39;&#39; Parse all inputs that are not Nodes to Tensors
        &#39;&#39;&#39;

        return [
            x if isinstance(x, _Node) else Tensor(x, name=str(x))
            for x in inputs
        ]

    def __repr__(self):
        return super(Function, self).__repr__() + f&#39;#{self.id}&#39;

    def _generate_tensor_name(self) -&gt; str:
        return &#39;Z&#39; + self.__repr__()

    @abstractmethod
    def forward(self) -&gt; ndarray:
        &#39;&#39;&#39; Implement forward pass of the function here.

        Use the `self.children` list to access the inputs.

        &#39;&#39;&#39;

        pass

    @abstractmethod
    def backward(self, idx: int, accum_grad: T) -&gt; T:
        &#39;&#39;&#39; Implement backward pass of the function here

        Compute the gradient of children[idx] w.r.t. output of the
        computation graph from the accumulated gradient (the gradient
        of the output of the function w.r.t. the output of the graph).

        Parameters:
        -----------
        - idx : int, the index of the children for which to compute the
         gradient w.r.t. output of the computation graph
        - accum_grad : T (Tensor or ndarray), the accumulated grad in the graph
         so far, you can otherwise think of it as the gradient of the output of
         the function w.r.t. the output of the graph.

            - `accum_grad` is Tensor if differentiantion is enabled
             (`DIFF_ENABLED`) and the children has opted for differentiation
             (`diff` is True), thus the computations will be recorded in the
             computation graph and higher-order derivatives could be computed.
            - otherwise, `accum_grad` is ndarray and the computations are not
             recorded; ndarrays are used since the computations with them are
             more efficient.

        Returns:
        --------
        - grad : T (Tensor or ndarray), the computed gradient of
         `self.children[idx]`

        &#39;&#39;&#39;

        pass

    def __call__(self) -&gt; Tensor:
        &#39;&#39;&#39; Executes cached forward pass
        &#39;&#39;&#39;

        # Forward pass
        self._output_placeholder.value = self.forward()
        return self._output_placeholder</code></pre>
                        </details>
                        <h3>Ancestors</h3>
                        <ul class="hlist">
                            <li>nujo.autodiff._node._Node</li>
                        </ul>
                        <h3>Subclasses</h3>
                        <ul class="hlist">
                            <li>nujo.autodiff._functions._activations._BinaryStep</li>
                            <li>nujo.autodiff._functions._activations._LeakyReLU</li>
                            <li>nujo.autodiff._functions._activations._ReLU</li>
                            <li>nujo.autodiff._functions._activations._Sigmoid</li>
                            <li>nujo.autodiff._functions._activations._Softmax</li>
                            <li>nujo.autodiff._functions._activations._Swish</li>
                            <li>nujo.autodiff._functions._activations._TanH</li>
                            <li>nujo.autodiff._functions._aggregate._InnerProd</li>
                            <li>nujo.autodiff._functions._aggregate._InnerSum</li>
                            <li>nujo.autodiff._functions._elementary._Addition</li>
                            <li>nujo.autodiff._functions._elementary._Logarithm</li>
                            <li>nujo.autodiff._functions._elementary._MatrixMul</li>
                            <li>nujo.autodiff._functions._elementary._Multiplication</li>
                            <li>nujo.autodiff._functions._elementary._Negation</li>
                            <li>nujo.autodiff._functions._elementary._Power</li>
                            <li>nujo.autodiff._functions._elementary._Reciprocal</li>
                        </ul>
                        <h3>Class variables</h3>
                        <dl>
                            <dt id="nujo.Function.T"><code class="name">var <span class="ident">T</span></code></dt>
                            <dd>
                                <div class="desc"></div>
                            </dd>
                        </dl>
                        <h3>Methods</h3>
                        <dl>
                            <dt id="nujo.Function.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>self, idx: int, accum_grad: ~T) -> ~T</span>
</code></dt>
                            <dd>
                                <div class="desc">
                                    <p>Implement backward pass of the function here</p>
                                    <p>Compute the gradient of children[idx] w.r.t. output of the
                                        computation graph from the accumulated gradient (the gradient
                                        of the output of the function w.r.t. the output of the graph).</p>
                                    <h2 id="parameters">Parameters:</h2>
                                    <ul>
                                        <li>idx : int, the index of the children for which to compute the
                                            gradient w.r.t. output of the computation graph</li>
                                        <li>
                                            <p>accum_grad : T (Tensor or ndarray), the accumulated grad in the graph
                                                so far, you can otherwise think of it as the gradient of the output of
                                                the function w.r.t. the output of the graph.</p>
                                            <ul>
                                                <li><code>accum_grad</code> is Tensor if differentiantion is enabled
                                                    (<code>DIFF_ENABLED</code>) and the children has opted for
                                                    differentiation
                                                    (<code>diff</code> is True), thus the computations will be recorded
                                                    in the
                                                    computation graph and higher-order derivatives could be computed.
                                                </li>
                                                <li>otherwise, <code>accum_grad</code> is ndarray and the computations
                                                    are not
                                                    recorded; ndarrays are used since the computations with them are
                                                    more efficient.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                    <h2 id="returns">Returns:</h2>
                                    <ul>
                                        <li>grad : T (Tensor or ndarray), the computed gradient of
                                            <code>self.children[idx]</code></li>
                                    </ul>
                                </div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">@abstractmethod
def backward(self, idx: int, accum_grad: T) -&gt; T:
    &#39;&#39;&#39; Implement backward pass of the function here

    Compute the gradient of children[idx] w.r.t. output of the
    computation graph from the accumulated gradient (the gradient
    of the output of the function w.r.t. the output of the graph).

    Parameters:
    -----------
    - idx : int, the index of the children for which to compute the
     gradient w.r.t. output of the computation graph
    - accum_grad : T (Tensor or ndarray), the accumulated grad in the graph
     so far, you can otherwise think of it as the gradient of the output of
     the function w.r.t. the output of the graph.

        - `accum_grad` is Tensor if differentiantion is enabled
         (`DIFF_ENABLED`) and the children has opted for differentiation
         (`diff` is True), thus the computations will be recorded in the
         computation graph and higher-order derivatives could be computed.
        - otherwise, `accum_grad` is ndarray and the computations are not
         recorded; ndarrays are used since the computations with them are
         more efficient.

    Returns:
    --------
    - grad : T (Tensor or ndarray), the computed gradient of
     `self.children[idx]`

    &#39;&#39;&#39;

    pass</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Function.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self) -> numpy.ndarray</span>
</code></dt>
                            <dd>
                                <div class="desc">
                                    <p>Implement forward pass of the function here.</p>
                                    <p>Use the <code>self.children</code> list to access the inputs.</p>
                                </div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">@abstractmethod
def forward(self) -&gt; ndarray:
    &#39;&#39;&#39; Implement forward pass of the function here.

    Use the `self.children` list to access the inputs.

    &#39;&#39;&#39;

    pass</code></pre>
                                </details>
                            </dd>
                        </dl>
                    </dd>
                    <dt id="nujo.Tensor"><code class="flex name class">
<span>class <span class="ident">Tensor</span></span>
<span>(</span><span>value: Union[ForwardRef('Tensor'), numpy.ndarray, List[numbers.Number], numbers.Number], diff=False, creator=None, name='Tensor')</span>
</code></dt>
                    <dd>
                        <div class="desc">
                            <p>Tensor - a multi-dimensional array</p>
                            <p>Tensors are the main units of data in nujo.
                                They "flow" in the computation graph. :)</p>
                            <p>Tensors can be either constants or trainable weights,
                                depending on whether gradients are computed for the given tensor.</p>
                            <h2 id="parameters">Parameters:</h2>
                            <ul>
                                <li>value : value, numerical value of the tensor</li>
                                <li>diff : boolean, whether to compute gradients for the tensor</li>
                                <li>creator : nujo function, that created this tensor;
                                    the only child of a tensor</li>
                                <li>name : string, representation of the tensor</li>
                            </ul>
                        </div>
                        <details class="source">
                            <summary>
                                <span>Expand source code</span>
                            </summary>
                            <pre><code class="python">class Tensor(_Node):
    &#39;&#39;&#39; Tensor - a multi-dimensional array

    Tensors are the main units of data in nujo.
    They &#34;flow&#34; in the computation graph. :)

    Tensors can be either constants or trainable weights,
    depending on whether gradients are computed for the given tensor.

    Parameters:
    -----------
     - value : value, numerical value of the tensor
     - diff : boolean, whether to compute gradients for the tensor
     - creator : nujo function, that created this tensor;
       the only child of a tensor
     - name : string, representation of the tensor

    &#39;&#39;&#39;
    def __init__(self,
                 value: Union[&#39;Tensor&#39;, ndarray, List[Number], Number],
                 diff=False,
                 creator=None,
                 name=&#39;Tensor&#39;):

        super(Tensor, self).__init__(*_if_not_none(creator), name=name)

        self._value: ndarray = None
        self.value = value  # set value

        self.diff = diff
        self.creator = creator

        # Outputs of the functions the current tensor is input to.
        # Used for backpropagation of the gradients.
        self.parents_outputs: List[&#39;Tensor&#39;] = []

        # Gradient of the current tensor
        self._grad: &#39;Tensor&#39; = None

        # Transposed tensor cache
        self._T: &#39;Tensor&#39; = None
        self._prev_value: ndarray = None

    @property
    def value(self):
        return self._value

    @value.setter
    def value(self, value: Union[&#39;Tensor&#39;, ndarray, List[Number], Number]):
        if isinstance(value, Tensor):
            self._value = value.value
        elif isinstance(value, ndarray):
            self._value = value
        else:
            self._value = array(value)

    @value.deleter
    def value(self):
        del self._value

    @property
    def grad(self) -&gt; &#39;Tensor&#39;:
        if self._grad is None:
            self._grad = Tensor(empty(self._value.shape),
                                name=f&#39;grad[{self.name}]&#39;)

        return self._grad

    # Shape and shape transformations

    @property
    def shape(self) -&gt; Tuple[int, ...]:
        return self._value.shape

    @property
    def T(self) -&gt; &#39;Tensor&#39;:
        # Only transpose if something has changed
        if (self._value != self._prev_value).any():
            self._T = self.transpose()
            self._prev_value = self._value

        return self._T

    def transpose(self, *dims: int) -&gt; &#39;Tensor&#39;:
        from nujo.autodiff._functions._transform import _Transpose
        return _Transpose(self, dims)()

    def reshape(self, *shape: int) -&gt; &#39;Tensor&#39;:
        from nujo.autodiff._functions._transform import _Reshape
        return _Reshape(self, shape)()

    def squeeze(self, dim=-1) -&gt; &#39;Tensor&#39;:
        if dim &lt; 0:
            num_dims = len(self._value.shape)

            if dim &lt; -num_dims:
                dim = num_dims
            else:
                dim += num_dims

        return self.reshape(*self._value.shape[:dim],
                            *self._value.shape[dim + 1:])

    def unsqueeze(self, dim=-1) -&gt; &#39;Tensor&#39;:
        if dim &lt; 0:
            num_dims = len(self._value.shape)

            if dim &lt; -num_dims:
                dim = 0
            else:
                if dim == -1:
                    dim += 1
                dim += num_dims

        return self.reshape(*self._value.shape[:dim], 1,
                            *self._value.shape[dim:])

    # Gradient computation

    def _compute_grad_wrt(self, poutput: &#39;Tensor&#39;) -&gt; Union[&#39;Tensor&#39;, ndarray]:
        &#39;&#39;&#39; Computes the gradient of `self` w.r.t. `poutput`

            In other words, this functions returns: dPoutput / dSelf.

        &#39;&#39;&#39;

        # Find out for which children the gradient should be computed
        idx = next(i for i, v in enumerate(poutput.creator.children)
                   if v is self)

        if poutput._grad.diff:
            # Pass a diff enabled tensor to the backward call,
            # thus recording grad computations in the computation
            # graph, which enables higher-order differentiation.
            update = poutput.creator.backward(idx, poutput._grad)

            # Check if `self` is scalar and needs to be averaged
            if self._value.shape != () and\
               self._value.shape[-1] == 1:

                # Record the mean in the computation graph
                from nujo.math.aggregate import mean
                update = mean(update, dim=-1, keepdim=True)

        else:
            # Do not leave a trace in the computation graph!
            # Use numpy arrays! :)
            update = poutput.creator.backward(idx, poutput._grad._value)

            # Check if `self` is scalar and needs to be averaged
            if self._value.shape != () and\
               self._value.shape[-1] == 1:

                update = update.mean(axis=-1, keepdims=True)

        return update

    def compute_grad(self) -&gt; None:
        if modes.DIFF_ENABLED and self.diff:

            # Make sure grad is Tensor (`grad property call`) and init value
            if self._grad is None:
                self.zero_grad(propagate=False)

            # Used only for test, to be removed
            assert self._grad._value.shape == self._value.shape
            assert (self._grad._value == 0).all()

            # Top-parent grad
            if len(self.parents_outputs) == 0:
                self._grad._value += 1
                return

            for poutput in self.parents_outputs:
                curr_grad = self._compute_grad_wrt(poutput)

                if self._grad.diff:
                    # Record grad computations in the computation graph
                    self._grad += curr_grad
                else:
                    self._grad._value += curr_grad

    def zero_grad(self, propagate=True) -&gt; None:
        self.grad._value.fill(0)

        if propagate:
            for poutput in self.parents_outputs:
                poutput.zero_grad()

    def backward(self, _debug=False) -&gt; None:
        &#39;&#39;&#39; It uses Breadth First Search to traverse the computation graph
        and compute the gradient for each differentiable Tensor in the graph.

        &#39;&#39;&#39;

        nodes_to_visit: List[&#39;Tensor&#39;] = [self]
        if _debug:
            i = 1

        while nodes_to_visit:
            node = nodes_to_visit.pop()
            node.compute_grad()

            if _debug:
                nstr = f&#39; [{i}]&#39;
                node.name += nstr if nstr not in node.name else &#39;&#39;
                i += 1

            if node.creator:
                for child in node.creator.children:
                    # Avoid visiting the same node twice
                    if all(child is not node for node in nodes_to_visit):
                        nodes_to_visit.insert(0, child)

    # Useful methods

    def all(self) -&gt; ndarray:
        return self._value.all()

    def any(self) -&gt; ndarray:
        return self._value.any()

    def __getitem__(self, position: Union[int, Tuple[int, ...]]):
        return Tensor(self._value[position],
                      diff=self.diff,
                      creator=self.creator,
                      name=f&#39;{self.name}[{position}]&#39;)

    def __setitem__(self, position: Union[int, Tuple[int, ...]],
                    value: Union[&#39;Tensor&#39;, ndarray, List[Number], Number]):

        self._value[position] = value

    def __hash__(self):
        return self.id

    # Static evaluation operator

    def __ilshift__(
            self, other: Union[&#39;Tensor&#39;, ndarray, List[Number],
                               Number]) -&gt; &#39;Tensor&#39;:
        &#39;&#39;&#39; In-place assignment operator: `&lt;&lt;=`

        Transfering key properties from `other` to `self`.
        Essentially a shortcut for:
            &gt;&gt;&gt; self.children = other.children
            &gt;&gt;&gt; self.creator = other.creator
            &gt;&gt;&gt; self.value = other.value
            &gt;&gt;&gt; self.grad = other.grad

        &#39;&#39;&#39;

        self.children = getattr(other, &#39;children&#39;, None)
        if self.children:
            try:
                self.children.remove(self)
            except ValueError:  # self is not in children
                pass

        self.creator = getattr(other, &#39;creator&#39;, None)
        if self.creator:
            try:
                self.creator.children.remove(self)
            except ValueError:  # self is not in children
                pass

        self._value = getattr(other, &#39;value&#39;, other)

        # Transfer the gradient
        self._grad = getattr(other, &#39;grad&#39;, None)

        return self

    # Comparison operations

    def __lt__(self, other):
        return self._value &lt; getattr(other, &#39;value&#39;, other)

    def __le__(self, other):
        return self._value &lt;= getattr(other, &#39;value&#39;, other)

    def __eq__(self, other):
        return self._value == getattr(other, &#39;value&#39;, other)

    def __ne__(self, other):
        return self._value != getattr(other, &#39;value&#39;, other)

    def __gt__(self, other):
        return self._value &gt; getattr(other, &#39;value&#39;, other)

    def __ge__(self, other):
        return self._value &gt;= getattr(other, &#39;value&#39;, other)

    # Arithmetic operations

    def __add__(self, other):
        from nujo.autodiff._functions._elementary import _Addition
        return _Addition(self, other)()

    def __radd__(self, other):
        return self.__add__(other)

    def __neg__(self):
        from nujo.autodiff._functions._elementary import _Negation
        return _Negation(self)()

    def __sub__(self, other):
        return self.__add__(other.__neg__())

    def __rsub__(self, other):
        return self.__neg__().__add__(other)

    def __mul__(self, other):
        from nujo.autodiff._functions._elementary import _Multiplication
        return _Multiplication(self, other)()

    def __rmul__(self, other):
        return self.__mul__(other)

    def __truediv__(self, other):
        from nujo.autodiff._functions._elementary import _Reciprocal
        return self.__mul__(_Reciprocal(other)())

    def __rtruediv__(self, other):
        from nujo.autodiff._functions._elementary import _Reciprocal
        return _Reciprocal(self)().__mul__(other)

    def __pow__(self, other):
        from nujo.autodiff._functions._elementary import _Power
        return _Power(self, other)()

    def __rpow__(self, other):
        from nujo.autodiff._functions._elementary import _Power
        return _Power(other, self)()

    # More complex arithmetic operations

    def __matmul__(self, other):
        from nujo.autodiff._functions._elementary import _MatrixMul
        return _MatrixMul(self, other)()

    def __rmatmul__(self, other):
        from nujo.autodiff._functions._elementary import _MatrixMul
        return _MatrixMul(other, self)()

    # Representations

    def __str__(self):
        # TODO: Come up with a better representation
        return self.__repr__() + &#39;\n&#39; + &#39;-&#39; * 32 + &#39;\n&#39; + str(self._value)</code></pre>
                        </details>
                        <h3>Ancestors</h3>
                        <ul class="hlist">
                            <li>nujo.autodiff._node._Node</li>
                        </ul>
                        <h3>Instance variables</h3>
                        <dl>
                            <dt id="nujo.Tensor.T"><code
                                    class="name">var <span class="ident">T</span> : <a title="nujo.autodiff.tensor.Tensor" href="autodiff/tensor.html#nujo.autodiff.tensor.Tensor">Tensor</a></code>
                            </dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">@property
def T(self) -&gt; &#39;Tensor&#39;:
    # Only transpose if something has changed
    if (self._value != self._prev_value).any():
        self._T = self.transpose()
        self._prev_value = self._value

    return self._T</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Tensor.grad"><code
                                    class="name">var <span class="ident">grad</span> : <a title="nujo.autodiff.tensor.Tensor" href="autodiff/tensor.html#nujo.autodiff.tensor.Tensor">Tensor</a></code>
                            </dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">@property
def grad(self) -&gt; &#39;Tensor&#39;:
    if self._grad is None:
        self._grad = Tensor(empty(self._value.shape),
                            name=f&#39;grad[{self.name}]&#39;)

    return self._grad</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Tensor.shape"><code
                                    class="name">var <span class="ident">shape</span> : Tuple[int, ...]</code></dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">@property
def shape(self) -&gt; Tuple[int, ...]:
    return self._value.shape</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Tensor.value"><code class="name">var <span class="ident">value</span></code>
                            </dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">@property
def value(self):
    return self._value</code></pre>
                                </details>
                            </dd>
                        </dl>
                        <h3>Methods</h3>
                        <dl>
                            <dt id="nujo.Tensor.all"><code class="name flex">
<span>def <span class="ident">all</span></span>(<span>self) -> numpy.ndarray</span>
</code></dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def all(self) -&gt; ndarray:
    return self._value.all()</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Tensor.any"><code class="name flex">
<span>def <span class="ident">any</span></span>(<span>self) -> numpy.ndarray</span>
</code></dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def any(self) -&gt; ndarray:
    return self._value.any()</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Tensor.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>self) -> NoneType</span>
</code></dt>
                            <dd>
                                <div class="desc">
                                    <p>It uses Breadth First Search to traverse the computation graph
                                        and compute the gradient for each differentiable Tensor in the graph.</p>
                                </div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def backward(self, _debug=False) -&gt; None:
    &#39;&#39;&#39; It uses Breadth First Search to traverse the computation graph
    and compute the gradient for each differentiable Tensor in the graph.

    &#39;&#39;&#39;

    nodes_to_visit: List[&#39;Tensor&#39;] = [self]
    if _debug:
        i = 1

    while nodes_to_visit:
        node = nodes_to_visit.pop()
        node.compute_grad()

        if _debug:
            nstr = f&#39; [{i}]&#39;
            node.name += nstr if nstr not in node.name else &#39;&#39;
            i += 1

        if node.creator:
            for child in node.creator.children:
                # Avoid visiting the same node twice
                if all(child is not node for node in nodes_to_visit):
                    nodes_to_visit.insert(0, child)</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Tensor.compute_grad"><code class="name flex">
<span>def <span class="ident">compute_grad</span></span>(<span>self) -> NoneType</span>
</code></dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def compute_grad(self) -&gt; None:
    if modes.DIFF_ENABLED and self.diff:

        # Make sure grad is Tensor (`grad property call`) and init value
        if self._grad is None:
            self.zero_grad(propagate=False)

        # Used only for test, to be removed
        assert self._grad._value.shape == self._value.shape
        assert (self._grad._value == 0).all()

        # Top-parent grad
        if len(self.parents_outputs) == 0:
            self._grad._value += 1
            return

        for poutput in self.parents_outputs:
            curr_grad = self._compute_grad_wrt(poutput)

            if self._grad.diff:
                # Record grad computations in the computation graph
                self._grad += curr_grad
            else:
                self._grad._value += curr_grad</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Tensor.reshape"><code class="name flex">
<span>def <span class="ident">reshape</span></span>(<span>self, *shape: int) -> <a title="nujo.autodiff.tensor.Tensor" href="autodiff/tensor.html#nujo.autodiff.tensor.Tensor">Tensor</a></span>
</code></dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def reshape(self, *shape: int) -&gt; &#39;Tensor&#39;:
    from nujo.autodiff._functions._transform import _Reshape
    return _Reshape(self, shape)()</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Tensor.squeeze"><code class="name flex">
<span>def <span class="ident">squeeze</span></span>(<span>self, dim=-1) -> <a title="nujo.autodiff.tensor.Tensor" href="autodiff/tensor.html#nujo.autodiff.tensor.Tensor">Tensor</a></span>
</code></dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def squeeze(self, dim=-1) -&gt; &#39;Tensor&#39;:
    if dim &lt; 0:
        num_dims = len(self._value.shape)

        if dim &lt; -num_dims:
            dim = num_dims
        else:
            dim += num_dims

    return self.reshape(*self._value.shape[:dim],
                        *self._value.shape[dim + 1:])</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Tensor.transpose"><code class="name flex">
<span>def <span class="ident">transpose</span></span>(<span>self, *dims: int) -> <a title="nujo.autodiff.tensor.Tensor" href="autodiff/tensor.html#nujo.autodiff.tensor.Tensor">Tensor</a></span>
</code></dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def transpose(self, *dims: int) -&gt; &#39;Tensor&#39;:
    from nujo.autodiff._functions._transform import _Transpose
    return _Transpose(self, dims)()</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Tensor.unsqueeze"><code class="name flex">
<span>def <span class="ident">unsqueeze</span></span>(<span>self, dim=-1) -> <a title="nujo.autodiff.tensor.Tensor" href="autodiff/tensor.html#nujo.autodiff.tensor.Tensor">Tensor</a></span>
</code></dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def unsqueeze(self, dim=-1) -&gt; &#39;Tensor&#39;:
    if dim &lt; 0:
        num_dims = len(self._value.shape)

        if dim &lt; -num_dims:
            dim = 0
        else:
            if dim == -1:
                dim += 1
            dim += num_dims

    return self.reshape(*self._value.shape[:dim], 1,
                        *self._value.shape[dim:])</code></pre>
                                </details>
                            </dd>
                            <dt id="nujo.Tensor.zero_grad"><code class="name flex">
<span>def <span class="ident">zero_grad</span></span>(<span>self, propagate=True) -> NoneType</span>
</code></dt>
                            <dd>
                                <div class="desc"></div>
                                <details class="source">
                                    <summary>
                                        <span>Expand source code</span>
                                    </summary>
                                    <pre><code class="python">def zero_grad(self, propagate=True) -&gt; None:
    self.grad._value.fill(0)

    if propagate:
        for poutput in self.parents_outputs:
            poutput.zero_grad()</code></pre>
                                </details>
                            </dd>
                        </dl>
                    </dd>
                    <dt id="nujo.no_diff"><code class="flex name class">
<span>class <span class="ident">no_diff</span></span>
</code></dt>
                    <dd>
                        <div class="desc">
                            <p>No Differentiation block</p>
                            <p>Creates a block of code where no differentiation is done.
                                a.k.a. No gradients are computed for whatever tensor.</p>
                        </div>
                        <details class="source">
                            <summary>
                                <span>Expand source code</span>
                            </summary>
                            <pre><code class="python">class no_diff():
    &#39;&#39;&#39; No Differentiation block

    Creates a block of code where no differentiation is done.
    a.k.a. No gradients are computed for whatever tensor.

    &#39;&#39;&#39;
    def __enter__(self):
        global DIFF_ENABLED
        DIFF_ENABLED = False

    def __exit__(self, type, value, traceback):
        global DIFF_ENABLED
        DIFF_ENABLED = True</code></pre>
                        </details>
                    </dd>
                </dl>
            </section>
        </article>
        <nav id="sidebar">
            <h1>Index</h1>
            <div class="toc">
                <ul></ul>
            </div>
            <ul id="index">
                <li>
                    <h3><a href="#header-submodules">Sub-modules</a></h3>
                    <ul>
                        <li><code><a title="nujo.autodiff" href="autodiff/index.html">nujo.autodiff</a></code></li>
                        <li><code><a title="nujo.flow" href="flow.html">nujo.flow</a></code></li>
                        <li><code><a title="nujo.init" href="init/index.html">nujo.init</a></code></li>
                        <li><code><a title="nujo.math" href="math/index.html">nujo.math</a></code></li>
                        <li><code><a title="nujo.nn" href="nn/index.html">nujo.nn</a></code></li>
                        <li><code><a title="nujo.objective" href="objective/index.html">nujo.objective</a></code></li>
                        <li><code><a title="nujo.optim" href="optim/index.html">nujo.optim</a></code></li>
                        <li><code><a title="nujo.utils" href="utils/index.html">nujo.utils</a></code></li>
                    </ul>
                </li>
                <li>
                    <h3><a href="#header-classes">Classes</a></h3>
                    <ul>
                        <li>
                            <h4><code><a title="nujo.Flow" href="#nujo.Flow">Flow</a></code></h4>
                            <ul class="">
                                <li><code><a title="nujo.Flow.append" href="#nujo.Flow.append">append</a></code></li>
                                <li><code><a title="nujo.Flow.copy" href="#nujo.Flow.copy">copy</a></code></li>
                                <li><code><a title="nujo.Flow.forward" href="#nujo.Flow.forward">forward</a></code></li>
                                <li><code><a title="nujo.Flow.parameters" href="#nujo.Flow.parameters">parameters</a></code>
                                </li>
                                <li><code><a title="nujo.Flow.pop" href="#nujo.Flow.pop">pop</a></code></li>
                            </ul>
                        </li>
                        <li>
                            <h4><code><a title="nujo.Function" href="#nujo.Function">Function</a></code></h4>
                            <ul class="">
                                <li><code><a title="nujo.Function.T" href="#nujo.Function.T">T</a></code></li>
                                <li><code><a title="nujo.Function.backward" href="#nujo.Function.backward">backward</a></code>
                                </li>
                                <li><code><a title="nujo.Function.forward" href="#nujo.Function.forward">forward</a></code>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <h4><code><a title="nujo.Tensor" href="#nujo.Tensor">Tensor</a></code></h4>
                            <ul class="two-column">
                                <li><code><a title="nujo.Tensor.T" href="#nujo.Tensor.T">T</a></code></li>
                                <li><code><a title="nujo.Tensor.all" href="#nujo.Tensor.all">all</a></code></li>
                                <li><code><a title="nujo.Tensor.any" href="#nujo.Tensor.any">any</a></code></li>
                                <li><code><a title="nujo.Tensor.backward" href="#nujo.Tensor.backward">backward</a></code>
                                </li>
                                <li><code><a title="nujo.Tensor.compute_grad" href="#nujo.Tensor.compute_grad">compute_grad</a></code>
                                </li>
                                <li><code><a title="nujo.Tensor.grad" href="#nujo.Tensor.grad">grad</a></code></li>
                                <li><code><a title="nujo.Tensor.reshape" href="#nujo.Tensor.reshape">reshape</a></code>
                                </li>
                                <li><code><a title="nujo.Tensor.shape" href="#nujo.Tensor.shape">shape</a></code></li>
                                <li><code><a title="nujo.Tensor.squeeze" href="#nujo.Tensor.squeeze">squeeze</a></code>
                                </li>
                                <li><code><a title="nujo.Tensor.transpose" href="#nujo.Tensor.transpose">transpose</a></code>
                                </li>
                                <li><code><a title="nujo.Tensor.unsqueeze" href="#nujo.Tensor.unsqueeze">unsqueeze</a></code>
                                </li>
                                <li><code><a title="nujo.Tensor.value" href="#nujo.Tensor.value">value</a></code></li>
                                <li><code><a title="nujo.Tensor.zero_grad" href="#nujo.Tensor.zero_grad">zero_grad</a></code>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <h4><code><a title="nujo.no_diff" href="#nujo.no_diff">no_diff</a></code></h4>
                        </li>
                    </ul>
                </li>
            </ul>
        </nav>
    </main>
    <footer id="footer">
        <p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
    </footer>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad()</script>
</body>

</html>