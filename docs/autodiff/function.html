<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>nujo.autodiff.function API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>nujo.autodiff.function</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from abc import abstractmethod
from numbers import Number
from typing import Dict, List, TypeVar, Union

from numpy import ndarray

from nujo.autodiff import modes
from nujo.autodiff._node import _Node
from nujo.autodiff.tensor import Tensor


class Function(_Node, object):
    &#39;&#39;&#39; Base Class for functions

    Functions are applied to tensors. They take multiple
    tensors as input and produces only one tensor as output.
    They do NOT change tensors in-place.

    Functions were also written so they reuse the input/output tensors
    when possible, which results in the computation graph being:
     - &#34;Dynamically defined, statically evaluated.&#34;
    taking the best from both worlds.

    Parameters:
    -----------
     - children : varargs, the inpute tensors
     - name : string, the name of the function

    &#39;&#39;&#39;

    _children_history: Dict[str, &#39;Function&#39;] = {}
    &#39;&#39;&#39; Cache where input tensors for
    the current function type are stored.
    &#39;&#39;&#39;

    _cache_hit = False
    &#39;&#39;&#39; Flag signaling cache hit/miss.
    &#39;&#39;&#39;

    T = TypeVar(&#39;T&#39;, Tensor, ndarray)

    def __init__(self,
                 *children: Union[Tensor, ndarray, List[Number], Number],
                 name=&#39;Function&#39;):

        if self._cache_hit:
            return

        super(Function, self).__init__(*children, name=name)

        # This output placeholder is reused when possible
        self._output_placeholder = Tensor(
            None,
            diff=any([x.diff for x in self.children]) and modes.DIFF_ENABLED,
            creator=self if modes.DIFF_ENABLED else None,
            name=self._generate_tensor_name())

        if modes.DIFF_ENABLED:  # If graph building is enabled.
            # Allocate space for parent&#39;s output (output placeholder)
            for child in self.children:
                child.parents_outputs.append(self._output_placeholder)

    def __new__(cls, *children: Union[Tensor, ndarray, List[Number], Number],
                **kwargs):
        &#39;&#39;&#39; Used to review the cache for hit and return the cached tensor
        or otherwise add the new tensor to the cache.
        &#39;&#39;&#39;

        # Only cache tensors that are in the computation graph
        if modes.DIFF_ENABLED:
            key = str(hash(cls))  # Inlcude the function type hash in the key
            # Include the arguments&#39; uids in the key
            key += &#39;&#39;.join((str(x.id) if isinstance(x, Tensor) else str(x)
                            for x in children))

            if key in cls._children_history:
                cls._cache_hit = True
                return cls._children_history[key]

            else:
                cls._cache_hit = False
                creator = super(Function, cls).__new__(cls)
                cls._children_history[key] = creator
                return creator

        # If the tensors are not in the computation graph,
        # perform standard python init
        else:
            cls._cache_hit = False
            return super(Function, cls).__new__(cls)

    def __repr__(self):
        return super(Function, self).__repr__() + f&#39;#{self.id}&#39;

    def _generate_tensor_name(self) -&gt; str:
        return &#39;Z&#39; + self.__repr__()

    @abstractmethod
    def forward(self) -&gt; ndarray:
        &#39;&#39;&#39; Implement forward pass of the function here.

        Use the `self.children` list to access the inputs.
        &#39;&#39;&#39;

        pass

    @abstractmethod
    def backward(self, idx: int, acumm_grad: T) -&gt; T:
        &#39;&#39;&#39; Implement backward pass of the function here

        Compute the gradient of children[idx] w.r.t. output of the
        computation graph from the acummulated gradient (the gradient
        of the output of the function w.r.t. the output of the graph).

        Parameters:
        -----------
        - idx : int, the index of the children for which to compute the
         gradient w.r.t. output of the computation graph
        - acumm_grad : T (Tensor or ndarray), the accumulated grad in the graph
         so far, you can otherwise think of it as the gradient of the output of
         the function w.r.t. the output of the graph.

            - `acumm_grad` is Tensor if differentiantion is enabled
             (`DIFF_ENABLED`) and the children has opted for differentiation
             (`diff` is True), thus the computations will be recorded in the
             computation graph and higher-order derivatives could be computed.
            - otherwise, `acumm_grad` is ndarray and the computations are not
             recorded; ndarrays are used since the computations with them are
             more efficient.

        Returns:
        --------
        - grad : T (Tensor or ndarray), the computed gradient of
         `self.children[idx]`

        &#39;&#39;&#39;

        pass

    def __call__(self) -&gt; Tensor:
        &#39;&#39;&#39; Executes cached forward pass
        &#39;&#39;&#39;

        # Forward pass
        self._output_placeholder.value = self.forward()
        return self._output_placeholder</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="nujo.autodiff.function.Function"><code class="flex name class">
<span>class <span class="ident">Function</span></span>
<span>(</span><span>*children: Union[nujo.autodiff.tensor.Tensor, numpy.ndarray, List[numbers.Number], numbers.Number], **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Base Class for functions</p>
<p>Functions are applied to tensors. They take multiple
tensors as input and produces only one tensor as output.
They do NOT change tensors in-place.</p>
<p>Functions were also written so they reuse the input/output tensors
when possible, which results in the computation graph being:
- "Dynamically defined, statically evaluated."
taking the best from both worlds.</p>
<h2 id="parameters">Parameters:</h2>
<ul>
<li>children : varargs, the inpute tensors</li>
<li>name : string, the name of the function</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Function(_Node, object):
    &#39;&#39;&#39; Base Class for functions

    Functions are applied to tensors. They take multiple
    tensors as input and produces only one tensor as output.
    They do NOT change tensors in-place.

    Functions were also written so they reuse the input/output tensors
    when possible, which results in the computation graph being:
     - &#34;Dynamically defined, statically evaluated.&#34;
    taking the best from both worlds.

    Parameters:
    -----------
     - children : varargs, the inpute tensors
     - name : string, the name of the function

    &#39;&#39;&#39;

    _children_history: Dict[str, &#39;Function&#39;] = {}
    &#39;&#39;&#39; Cache where input tensors for
    the current function type are stored.
    &#39;&#39;&#39;

    _cache_hit = False
    &#39;&#39;&#39; Flag signaling cache hit/miss.
    &#39;&#39;&#39;

    T = TypeVar(&#39;T&#39;, Tensor, ndarray)

    def __init__(self,
                 *children: Union[Tensor, ndarray, List[Number], Number],
                 name=&#39;Function&#39;):

        if self._cache_hit:
            return

        super(Function, self).__init__(*children, name=name)

        # This output placeholder is reused when possible
        self._output_placeholder = Tensor(
            None,
            diff=any([x.diff for x in self.children]) and modes.DIFF_ENABLED,
            creator=self if modes.DIFF_ENABLED else None,
            name=self._generate_tensor_name())

        if modes.DIFF_ENABLED:  # If graph building is enabled.
            # Allocate space for parent&#39;s output (output placeholder)
            for child in self.children:
                child.parents_outputs.append(self._output_placeholder)

    def __new__(cls, *children: Union[Tensor, ndarray, List[Number], Number],
                **kwargs):
        &#39;&#39;&#39; Used to review the cache for hit and return the cached tensor
        or otherwise add the new tensor to the cache.
        &#39;&#39;&#39;

        # Only cache tensors that are in the computation graph
        if modes.DIFF_ENABLED:
            key = str(hash(cls))  # Inlcude the function type hash in the key
            # Include the arguments&#39; uids in the key
            key += &#39;&#39;.join((str(x.id) if isinstance(x, Tensor) else str(x)
                            for x in children))

            if key in cls._children_history:
                cls._cache_hit = True
                return cls._children_history[key]

            else:
                cls._cache_hit = False
                creator = super(Function, cls).__new__(cls)
                cls._children_history[key] = creator
                return creator

        # If the tensors are not in the computation graph,
        # perform standard python init
        else:
            cls._cache_hit = False
            return super(Function, cls).__new__(cls)

    def __repr__(self):
        return super(Function, self).__repr__() + f&#39;#{self.id}&#39;

    def _generate_tensor_name(self) -&gt; str:
        return &#39;Z&#39; + self.__repr__()

    @abstractmethod
    def forward(self) -&gt; ndarray:
        &#39;&#39;&#39; Implement forward pass of the function here.

        Use the `self.children` list to access the inputs.
        &#39;&#39;&#39;

        pass

    @abstractmethod
    def backward(self, idx: int, acumm_grad: T) -&gt; T:
        &#39;&#39;&#39; Implement backward pass of the function here

        Compute the gradient of children[idx] w.r.t. output of the
        computation graph from the acummulated gradient (the gradient
        of the output of the function w.r.t. the output of the graph).

        Parameters:
        -----------
        - idx : int, the index of the children for which to compute the
         gradient w.r.t. output of the computation graph
        - acumm_grad : T (Tensor or ndarray), the accumulated grad in the graph
         so far, you can otherwise think of it as the gradient of the output of
         the function w.r.t. the output of the graph.

            - `acumm_grad` is Tensor if differentiantion is enabled
             (`DIFF_ENABLED`) and the children has opted for differentiation
             (`diff` is True), thus the computations will be recorded in the
             computation graph and higher-order derivatives could be computed.
            - otherwise, `acumm_grad` is ndarray and the computations are not
             recorded; ndarrays are used since the computations with them are
             more efficient.

        Returns:
        --------
        - grad : T (Tensor or ndarray), the computed gradient of
         `self.children[idx]`

        &#39;&#39;&#39;

        pass

    def __call__(self) -&gt; Tensor:
        &#39;&#39;&#39; Executes cached forward pass
        &#39;&#39;&#39;

        # Forward pass
        self._output_placeholder.value = self.forward()
        return self._output_placeholder</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>nujo.autodiff._node._Node</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>nujo.autodiff._functions._activations._BinaryStep</li>
<li>nujo.autodiff._functions._activations._LeakyReLU</li>
<li>nujo.autodiff._functions._activations._ReLU</li>
<li>nujo.autodiff._functions._activations._Sigmoid</li>
<li>nujo.autodiff._functions._activations._Softmax</li>
<li>nujo.autodiff._functions._activations._Swish</li>
<li>nujo.autodiff._functions._activations._TanH</li>
<li>nujo.autodiff._functions._aggregate._InnerProd</li>
<li>nujo.autodiff._functions._aggregate._InnerSum</li>
<li>nujo.autodiff._functions._elementary._Addition</li>
<li>nujo.autodiff._functions._elementary._Logarithm</li>
<li>nujo.autodiff._functions._elementary._MatrixMul</li>
<li>nujo.autodiff._functions._elementary._Multiplication</li>
<li>nujo.autodiff._functions._elementary._Negation</li>
<li>nujo.autodiff._functions._elementary._Power</li>
<li>nujo.autodiff._functions._elementary._Reciprocal</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="nujo.autodiff.function.Function.T"><code class="name">var <span class="ident">T</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="nujo.autodiff.function.Function.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>self, idx: int, acumm_grad: ~T) -> ~T</span>
</code></dt>
<dd>
<div class="desc"><p>Implement backward pass of the function here</p>
<p>Compute the gradient of children[idx] w.r.t. output of the
computation graph from the acummulated gradient (the gradient
of the output of the function w.r.t. the output of the graph).</p>
<h2 id="parameters">Parameters:</h2>
<ul>
<li>idx : int, the index of the children for which to compute the
gradient w.r.t. output of the computation graph</li>
<li>
<p>acumm_grad : T (Tensor or ndarray), the accumulated grad in the graph
so far, you can otherwise think of it as the gradient of the output of
the function w.r.t. the output of the graph.</p>
<ul>
<li><code>acumm_grad</code> is Tensor if differentiantion is enabled
(<code>DIFF_ENABLED</code>) and the children has opted for differentiation
(<code>diff</code> is True), thus the computations will be recorded in the
computation graph and higher-order derivatives could be computed.</li>
<li>otherwise, <code>acumm_grad</code> is ndarray and the computations are not
recorded; ndarrays are used since the computations with them are
more efficient.</li>
</ul>
</li>
</ul>
<h2 id="returns">Returns:</h2>
<ul>
<li>grad : T (Tensor or ndarray), the computed gradient of
<code>self.children[idx]</code></li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def backward(self, idx: int, acumm_grad: T) -&gt; T:
    &#39;&#39;&#39; Implement backward pass of the function here

    Compute the gradient of children[idx] w.r.t. output of the
    computation graph from the acummulated gradient (the gradient
    of the output of the function w.r.t. the output of the graph).

    Parameters:
    -----------
    - idx : int, the index of the children for which to compute the
     gradient w.r.t. output of the computation graph
    - acumm_grad : T (Tensor or ndarray), the accumulated grad in the graph
     so far, you can otherwise think of it as the gradient of the output of
     the function w.r.t. the output of the graph.

        - `acumm_grad` is Tensor if differentiantion is enabled
         (`DIFF_ENABLED`) and the children has opted for differentiation
         (`diff` is True), thus the computations will be recorded in the
         computation graph and higher-order derivatives could be computed.
        - otherwise, `acumm_grad` is ndarray and the computations are not
         recorded; ndarrays are used since the computations with them are
         more efficient.

    Returns:
    --------
    - grad : T (Tensor or ndarray), the computed gradient of
     `self.children[idx]`

    &#39;&#39;&#39;

    pass</code></pre>
</details>
</dd>
<dt id="nujo.autodiff.function.Function.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self) -> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>Implement forward pass of the function here.</p>
<p>Use the <code>self.children</code> list to access the inputs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def forward(self) -&gt; ndarray:
    &#39;&#39;&#39; Implement forward pass of the function here.

    Use the `self.children` list to access the inputs.
    &#39;&#39;&#39;

    pass</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="nujo.autodiff" href="index.html">nujo.autodiff</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="nujo.autodiff.function.Function" href="#nujo.autodiff.function.Function">Function</a></code></h4>
<ul class="">
<li><code><a title="nujo.autodiff.function.Function.T" href="#nujo.autodiff.function.Function.T">T</a></code></li>
<li><code><a title="nujo.autodiff.function.Function.backward" href="#nujo.autodiff.function.Function.backward">backward</a></code></li>
<li><code><a title="nujo.autodiff.function.Function.forward" href="#nujo.autodiff.function.Function.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>